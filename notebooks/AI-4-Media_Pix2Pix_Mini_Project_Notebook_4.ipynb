{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini Project - Take 4\n",
    "By Marissa Beaty"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was taken and adapted from a website run by Aryan Esfandiari, available here: https://www.aryan.no/post/pix2pix/pix2pix/.  It utilizes Pytorch Lightning to run the model more efficiently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch: 1.13.1\n",
      "Pytorch Vision: 0.14.1\n",
      "Pytorch Lightning: 1.9.4\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import dill as pickle\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import pytorch_lightning as pl # pip install pytorch-lightning\n",
    "from torchmetrics.functional import (peak_signal_noise_ratio, structural_similarity_index_measure, accuracy) # pip install torchmetrics\n",
    "from torchsummary import summary # pip install torchsummary\n",
    "import segmentation_models_pytorch as smp # pip install segmentation-models-pytorch\n",
    "\n",
    "print(f'Pytorch: {torch.__version__}')\n",
    "print(f'Pytorch Vision: {torchvision.__version__}')\n",
    "print(f'Pytorch Lightning: {pl.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TRAIN_PATH = 'Image_pairs/Training'\n",
    "DATASET_TEST_PATH = 'Image_pairs/Test/'\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "TRAIN_IMAGE_SIZE = 512\n",
    "VAL_BATCH_SIZE = 16\n",
    "VAL_IMAGE_SIZE = 512\n",
    "TEST_BATCH_SIZE = 20\n",
    "TEST_IMAGE_SIZE = 512\n",
    "LAMBDA = 100\n",
    "NUM_EPOCH = 100\n",
    "AVAILABLE_GPUS = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorizontalSplit(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Split the image in two, horizontally\n",
    "    E.g. (1, 3, 256, 512) -> (2, 3, 256, 256) if the ratio is 0.5\n",
    "    \"\"\"\n",
    "    def __init__(self, ratio=0.5):\n",
    "        super(HorizontalSplit, self).__init__()\n",
    "        self.ratio = ratio\n",
    "    \n",
    "    def forward(self, image):\n",
    "        w, h = image.size\n",
    "        idx = int(w * self.ratio)\n",
    "        left = TF.crop(image, top=0, left=0, height=h, width=idx)\n",
    "        right = TF.crop(image, top=0, left=idx, height=h, width=idx)\n",
    "        return left, right      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SketchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for Edges-to-RGB datasets such as edges2handbags and edges2shoes\n",
    "    Download: http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/\n",
    "    \"\"\"\n",
    "    def __init__(self, filenames, split='train', transform=None):\n",
    "        self.filenames = filenames\n",
    "        self.split = split\n",
    "        \n",
    "        # Data transform\n",
    "        if not transform:\n",
    "            if self.split == 'train':\n",
    "                self.transform = transforms.Compose([\n",
    "                    HorizontalSplit(0.5),\n",
    "                    transforms.Lambda(lambda images: torch.stack([transforms.ToTensor()(item) for item in images])),\n",
    "                    transforms.CenterCrop((TRAIN_IMAGE_SIZE, TRAIN_IMAGE_SIZE)),\n",
    "                    transforms.RandomVerticalFlip(),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomRotation(45, interpolation=torchvision.transforms.InterpolationMode.BILINEAR, fill=1),\n",
    "                ])\n",
    "            elif self.split == 'val':\n",
    "                self.transform = transforms.Compose([\n",
    "                    HorizontalSplit(0.5),\n",
    "                    transforms.Lambda(lambda images: torch.stack([transforms.ToTensor()(item) for item in images])),\n",
    "                    transforms.CenterCrop((VAL_IMAGE_SIZE, VAL_IMAGE_SIZE)),\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = transforms.Compose([\n",
    "                    HorizontalSplit(0.5),\n",
    "                    transforms.Lambda(lambda images: torch.stack([transforms.ToTensor()(item) for item in images])),\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.filenames[idx])\n",
    "        image = self.transform(image)\n",
    "        image, target = torch.split(image, 1)\n",
    "        image = torch.squeeze(image)\n",
    "        # Convert image (edges) to real grayscale e.g. 1-D\n",
    "        image = TF.rgb_to_grayscale(image)\n",
    "        target = torch.squeeze(target)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 812 - Validation dataset: 91\n"
     ]
    }
   ],
   "source": [
    "filenames = sorted(glob.glob('Image_pairs/Training/*.png'))\n",
    "train_filenames, val_filenames = train_test_split(filenames, test_size=0.1)\n",
    "print(f'Train dataset: {len(train_filenames)} - Validation dataset: {len(val_filenames)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'SketchDataset.__init__.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39mTRAIN_BATCH_SIZE, num_workers\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m fig, (ax1, ax2) \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m18\u001b[39m, \u001b[39m18\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39;49m(train_dataloader))\n\u001b[1;32m      8\u001b[0m image, target \u001b[39m=\u001b[39m data\n\u001b[1;32m     10\u001b[0m grid_image \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mmake_grid(image, nrow\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:435\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[1;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:381\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 381\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1034\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1027\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m   1035\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1036\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_posix\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(process_obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch(process_obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/popen_spawn_posix.py:47\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, fp)\n\u001b[0;32m---> 47\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, fp)\n\u001b[1;32m     48\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     59\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'SketchDataset.__init__.<locals>.<lambda>'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training dataset and datalaoder\n",
    "train_dataset = SketchDataset(train_filenames, split='train')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=16, shuffle=True, drop_last=True)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 18))\n",
    "\n",
    "data = next(iter(train_dataloader))\n",
    "image, target = data\n",
    "\n",
    "grid_image = torchvision.utils.make_grid(image, nrow=16)\n",
    "ax1.imshow(grid_image.permute(1, 2, 0))\n",
    "grid_image = torchvision.utils.make_grid(target, nrow=16)\n",
    "ax2.imshow(grid_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation dataset and dataloader\n",
    "val_dataset = SketchDataset(val_filenames, split='val')\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, num_workers=8, shuffle=False)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 18))\n",
    "\n",
    "data = next(iter(val_dataloader))\n",
    "image, target = data\n",
    "grid_image = torchvision.utils.make_grid(image, nrow=16)\n",
    "ax1.imshow(grid_image.permute(1, 2, 0))\n",
    "grid_image = torchvision.utils.make_grid(target, nrow=16)\n",
    "ax2.imshow(grid_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filenames = sorted(glob.glob(f'{DATASET_TEST_PATH}/*.jpg'))\n",
    "test_dataset = SketchDataset(test_filenames, split='test')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "data = next(iter(test_dataloader))\n",
    "image, target = data\n",
    "grid_image = torchvision.utils.make_grid(image, nrow=10)\n",
    "plt.imshow(grid_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, dropout_p=0.4):\n",
    "        super(Generator, self).__init__()\n",
    "        self.dropout_p = dropout_p\n",
    "        # Load Unet with Resnet34 Embedding from SMP library. Pre-trained on Imagenet\n",
    "        self.unet = smp.Unet(encoder_name=\"resnet34\", encoder_weights=\"imagenet\", \n",
    "                             in_channels=1, classes=3, activation=None)\n",
    "        # Adding two layers of Dropout as the original Unet doesn't have any\n",
    "        # This will be used to feed noise into the networking during both training and evaluation\n",
    "        # These extra layers will be added on decoder part where 2D transposed convolution is occured\n",
    "        for idx in range(1, 3):\n",
    "            self.unet.decoder.blocks[idx].conv1.add_module('3', nn.Dropout2d(p=self.dropout_p))\n",
    "        # Disabling in-place ReLU as to avoid in-place operations as it will\n",
    "        # cause issues for double backpropagation on the same graph\n",
    "        for module in self.unet.modules():\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                module.inplace = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.unet(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "summary(generator, input_size=(1, 256, 256), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, dropout_p=0.4):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dropout_p = dropout_p\n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(4, 128, 3, stride=2, padding=2)),\n",
    "          ('bn1', nn.BatchNorm2d(128)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('conv2', nn.Conv2d(128, 256, 3, stride=2, padding=2)),\n",
    "          ('bn2', nn.BatchNorm2d(256)),\n",
    "          ('relu2', nn.ReLU()),\n",
    "          ('conv3', nn.Conv2d(256, 512, 3)),\n",
    "          ('dropout3', nn.Dropout2d(p=self.dropout_p)),\n",
    "          ('bn3', nn.BatchNorm2d(512)),\n",
    "          ('relu3', nn.ReLU()),\n",
    "          ('conv4', nn.Conv2d(512, 1024, 3)),\n",
    "          ('dropout4', nn.Dropout2d(p=self.dropout_p)),\n",
    "          ('bn4', nn.BatchNorm2d(1024)),\n",
    "          ('relu4', nn.ReLU()),\n",
    "          ('conv5', nn.Conv2d(1024, 512, 3, stride=2, padding=2)),\n",
    "          ('dropout5', nn.Dropout2d(p=self.dropout_p)),\n",
    "          ('bn5', nn.BatchNorm2d(512)),\n",
    "          ('relu5', nn.ReLU()),\n",
    "          ('conv6', nn.Conv2d(512, 256, 3, stride=2, padding=2)),\n",
    "          ('dropout6', nn.Dropout2d(p=self.dropout_p)),\n",
    "          ('bn6', nn.BatchNorm2d(256)),\n",
    "          ('relu6', nn.ReLU()),\n",
    "          ('conv7', nn.Conv2d(256, 128, 3)),\n",
    "          ('bn7', nn.BatchNorm2d(128)),\n",
    "          ('relu7', nn.ReLU()),\n",
    "          ('conv8', nn.Conv2d(128, 1, 3)),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: {Grayscale image, RGB target} concatenated along the channel-axis\n",
    "        x = self.model(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return  x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "summary(discriminator, input_size=(4, 256, 256), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pix2Pix(pl.LightningModule):\n",
    "    def __init__(self, generator_dropout_p=0.4, discriminator_dropout_p=0.4, generator_lr=1e-3, discriminator_lr=1e-6, \n",
    "                 weight_decay=1e-5, lr_scheduler_T_0=1e3, lr_scheduler_T_mult=2):\n",
    "        super(Pix2Pix, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # Important to disable automatic optimization as it \n",
    "        # will be done manually as there are two optimizators\n",
    "        self.automatic_optimization = False\n",
    "        self.generator_lr = generator_lr               # Generator learning rate\n",
    "        self.discriminator_lr = discriminator_lr       # Discriminator learning rate\n",
    "        self.weight_decay = weight_decay               # Weight decay e.g. L2 regularization\n",
    "        self.lr_scheduler_T_0 = lr_scheduler_T_0       # Optimizer initial restart step number\n",
    "        self.lr_scheduler_T_mult = lr_scheduler_T_mult # Optimizer restart step number factor\n",
    "        \n",
    "        # Models\n",
    "        self.generator = Generator(dropout_p=generator_dropout_p)\n",
    "        self.discriminator = Discriminator(dropout_p=discriminator_dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.generator(x)\n",
    "    \n",
    "    def generator_loss(self, prediction_image, target_image, prediction_label, target_label):\n",
    "        \"\"\"\n",
    "        Generator loss (a combination of): \n",
    "            1 - Binary Cross-Entropy\n",
    "                Between predicted labels (generated by the discriminator) and target labels which is all 1s\n",
    "            2 - L1 / Mean Absolute Error (weighted by lambda)\n",
    "                Between generated image and target image\n",
    "            3 - L2 / Mean Squared Error (weighted by lambda)\n",
    "                Between generated image and target image\n",
    "        \"\"\"\n",
    "        bce_loss = F.binary_cross_entropy(prediction_label, target_label)\n",
    "        l1_loss = F.l1_loss(prediction_image, target_image)\n",
    "        mse_loss = F.mse_loss(prediction_image, target_image)\n",
    "        return bce_loss, l1_loss, mse_loss\n",
    "    \n",
    "    def discriminator_loss(self, prediction_label, target_label):\n",
    "        \"\"\"\n",
    "        Discriminator loss: \n",
    "            1 - Binary Cross-Entropy\n",
    "                Between predicted labels (generated by the discriminator) and target labels\n",
    "                The target would be all 0s if the input of the discriminator is the generated image (generator)\n",
    "                The target would be all 1s if the input of the discriminator is the target image (dataloader)\n",
    "        \"\"\"\n",
    "        bce_loss = F.binary_cross_entropy(prediction_label, target_label)\n",
    "        return bce_loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Using Adam optimizer for both generator and discriminator including L2 regularization\n",
    "        Both would have different initial learning rates\n",
    "        Stochastic Gradient Descent with Warm Restarts is also added as learning scheduler (https://arxiv.org/abs/1608.03983)\n",
    "        \"\"\"\n",
    "        # Optimizers\n",
    "        generator_optimizer = torch.optim.Adam(self.generator.parameters(), lr=self.generator_lr, weight_decay=self.weight_decay)\n",
    "        discriminator_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=self.discriminator_lr, weight_decay=self.weight_decay)\n",
    "        # Learning Scheduler\n",
    "        genertator_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(generator_optimizer, T_0=self.lr_scheduler_T_0, T_mult=self.lr_scheduler_T_mult)\n",
    "        discriminator_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(discriminator_optimizer, T_0=self.lr_scheduler_T_0, T_mult=self.lr_scheduler_T_mult)\n",
    "        return [generator_optimizer, discriminator_optimizer], [genertator_lr_scheduler, discriminator_lr_scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Optimizers\n",
    "        generator_optimizer, discriminator_optimizer = self.optimizers()\n",
    "        generator_lr_scheduler, discriminator_lr_scheduler = self.lr_schedulers()\n",
    "        \n",
    "        image, target = batch\n",
    "        image_i, image_j = torch.split(image, TRAIN_BATCH_SIZE // 2)\n",
    "        target_i, target_j = torch.split(target, TRAIN_BATCH_SIZE // 2)\n",
    "        \n",
    "        ######################################\n",
    "        #  Discriminator Loss and Optimizer  #\n",
    "        ######################################\n",
    "        # Generator Feed-Forward\n",
    "        generator_prediction = self.forward(image_i)\n",
    "        generator_prediction = torch.clip(generator_prediction, 0, 1)\n",
    "        # Discriminator Feed-Forward\n",
    "        discriminator_prediction_real = self.discriminator(torch.cat((image_i, target_i), dim=1))\n",
    "        discriminator_prediction_fake = self.discriminator(torch.cat((image_i, generator_prediction), dim=1))\n",
    "        # Discriminator Loss\n",
    "        discriminator_label_real = self.discriminator_loss(discriminator_prediction_real, \n",
    "                                                           torch.ones_like(discriminator_prediction_real))\n",
    "        discriminator_label_fake = self.discriminator_loss(discriminator_prediction_fake,\n",
    "                                                           torch.zeros_like(discriminator_prediction_fake))\n",
    "        discriminator_loss = discriminator_label_real + discriminator_label_fake\n",
    "        # Discriminator Optimizer\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "        discriminator_lr_scheduler.step()\n",
    "        \n",
    "        ##################################\n",
    "        #  Generator Loss and Optimizer  #\n",
    "        ##################################\n",
    "        #  Generator Feed-Forward\n",
    "        generator_prediction = self.forward(image_j)\n",
    "        generator_prediction = torch.clip(generator_prediction, 0, 1)\n",
    "        # Discriminator Feed-Forward\n",
    "        discriminator_prediction_fake = self.discriminator(torch.cat((image_j, generator_prediction), dim=1))\n",
    "        # Generator loss\n",
    "        generator_bce_loss, generator_l1_loss, generator_mse_loss = self.generator_loss(generator_prediction, target_j,\n",
    "                                                                                        discriminator_prediction_fake,\n",
    "                                                                                        torch.ones_like(discriminator_prediction_fake))\n",
    "        generator_loss = generator_bce_loss + (generator_l1_loss * LAMBDA) + (generator_mse_loss * LAMBDA)\n",
    "        # Generator Optimizer\n",
    "        generator_optimizer.zero_grad()\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "        generator_lr_scheduler.step()\n",
    "        \n",
    "        # Progressbar and Logging\n",
    "        loss = OrderedDict({'train_g_bce_loss': generator_bce_loss, 'train_g_l1_loss': generator_l1_loss, 'train_g_mse_loss': generator_mse_loss,\n",
    "                            'train_g_loss': generator_loss, 'train_d_loss': discriminator_loss,\n",
    "                            'train_g_lr': generator_lr_scheduler.get_last_lr()[0], 'train_d_lr': discriminator_lr_scheduler.get_last_lr()[0]})\n",
    "        self.log_dict(loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        image, target = batch\n",
    "        \n",
    "        # Generator Feed-Forward\n",
    "        generator_prediction = self.forward(image)\n",
    "        generator_prediction = torch.clip(generator_prediction, 0, 1)\n",
    "        # Generator Metrics\n",
    "        generator_psnr = psnr(generator_prediction, target)\n",
    "        generator_ssim = ssim(generator_prediction, target)\n",
    "        discriminator_prediction_fake = self.discriminator(torch.cat((image, generator_prediction), dim=1))\n",
    "        generator_accuracy = accuracy(discriminator_prediction_fake, torch.ones_like(discriminator_prediction_fake, dtype=torch.int32))\n",
    "        \n",
    "        # Discriminator Feed-Forward\n",
    "        discriminator_prediction_real = self.discriminator(torch.cat((image, target), dim=1))\n",
    "        discriminator_prediction_fake = self.discriminator(torch.cat((image, generator_prediction), dim=1))\n",
    "        # Discriminator Metrics\n",
    "        discriminator_accuracy = accuracy(discriminator_prediction_real, torch.ones_like(discriminator_prediction_real, dtype=torch.int32)) * 0.5 + \\\n",
    "                                accuracy(discriminator_prediction_fake, torch.zeros_like(discriminator_prediction_fake, dtype=torch.int32)) * 0.5\n",
    "            \n",
    "        # Progressbar and Logging\n",
    "        metrics = OrderedDict({'val_g_psnr': generator_psnr, 'val_g_ssim': generator_ssim,\n",
    "                               'val_g_accuracy': generator_accuracy, 'val_d_accuracy': discriminator_accuracy})\n",
    "        \n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochInference(pl.callbacks.base.Callback):\n",
    "    \"\"\"\n",
    "    Callback on each end of training epoch\n",
    "    The callback will do inference on test dataloader based on corresponding checkpoints\n",
    "    The results will be saved as an image with 4-rows:\n",
    "        1 - Input image e.g. grayscale edged input\n",
    "        2 - Ground-truth\n",
    "        3 - Single inference\n",
    "        4 - Mean of hundred accumulated inference\n",
    "    Note that the inference have a noise factor that will generate different output on each execution\n",
    "    \"\"\"\n",
    "    def __init__(self, dataloader, *args, **kwargs):\n",
    "        super(EpochInference, self).__init__(*args, **kwargs)\n",
    "        self.dataloader = dataloader\n",
    "        \n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        super(EpochInference, self).on_train_epoch_end(trainer, pl_module)\n",
    "        data = next(iter(self.dataloader))\n",
    "        image, target = data\n",
    "        image = image.cuda()\n",
    "        target = target.cuda()\n",
    "        with torch.no_grad():\n",
    "            # Take average of multiple inference as there is a random noise\n",
    "            # Single\n",
    "            reconstruction_init = pl_module.forward(image)\n",
    "            reconstruction_init = torch.clip(reconstruction_init, 0, 1)\n",
    "            # Mean\n",
    "            reconstruction_mean = torch.stack([pl_module.forward(image) for _ in range(100)])\n",
    "            reconstruction_mean = torch.clip(reconstruction_mean, 0, 1)\n",
    "            reconstruction_mean = torch.mean(reconstruction_mean, dim=0)\n",
    "        # Grayscale 1-D to 3-D\n",
    "        image = torch.stack([image for _ in range(3)], dim=1)\n",
    "        image = torch.squeeze(image)\n",
    "        grid_image = torchvision.utils.make_grid(torch.cat([image, target, reconstruction_init, reconstruction_mean], dim=0), nrow=20)\n",
    "        torchvision.utils.save_image(grid_image, fp=f'{trainer.log_dir}/epoch-{trainer.current_epoch:04}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_inference_callback = EpochInference(test_dataloader)\n",
    "checkpoint_callback = pl.callbacks.model_checkpoint.ModelCheckpoint()\n",
    "\n",
    "model = Pix2Pix(generator_dropout_p=0.4, discriminator_dropout_p=0.25,\n",
    "                generator_lr=1e-3, discriminator_lr=1e-5, \n",
    "                lr_scheduler_T_0=len(train_dataloader) * 2)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=NUM_EPOCH, val_check_interval=0.5, default_root_dir='./pix2pix/', \n",
    "                     callbacks=[epoch_inference_callback, checkpoint_callback])\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = sorted(glob.glob('./pix2pix/checkpoints/*.ckpt'), key=os.path.getmtime)[0]\n",
    "checkpoint = torch.load(checkpoint)\n",
    "model = Pix2Pix()\n",
    "model.load_state_dict(checkpoint['state_dict'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(test_dataloader))\n",
    "image, _ = data\n",
    "with torch.no_grad():\n",
    "    reconstruction = model(image)\n",
    "    reconstruction = torch.clip(reconstruction, 0, 1)\n",
    "\n",
    "image = torch.stack([image for _ in range(3)], dim=1)\n",
    "image = torch.squeeze(image)\n",
    "grid_image = torchvision.utils.make_grid(torch.cat([image, reconstruction]), nrow=20)\n",
    "plt.figure(figsize=(24, 24))\n",
    "plt.imshow(grid_image.permute(1, 2, 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code reached an error in cell 7 saying \"Can't pickle local object 'SketchDataset.__init__.<locals>.<lambda>'.\" Doing research on this specific error, I found that the pickle function does better when using a global object rather than a local object. I attempted to adjust the code using example 2 from this website: https://www.pythonpool.com/cant-pickle-local-object/. Unfortunately, this did not solve the error. I then found that sometimes where Pickle doesn't work, Dill can be used in replace. I did attempt to import dill and implement it where Pickle was used, but came to the same error and went back to the original code. I attempted to do more research on a solution for this error, but without sufficient success. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:27:35) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "867332fcff6d9ef60802a7aa280ca2e67d2157da374b31f12b9e31ff4c97c0fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
