{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> Artificial Intelligence for the Media"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "Mini-Project\n",
    "By Marissa Beaty"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this Mini-Project, I will be training a Pix2Pix model on a self-made dataset composed of outputs and inputs. The outputs for this project are a collection of 1,290 images collected from seven different Pinterest searches using the PinDown Google extension. The inputs for this project are those same images run through a Canny edge detection function to leave only outlines of the images defining characteristics. The below code will train a Pix2Pix model on this dataset. This model will be in Pytorch, and is borrowed from the research team, Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros who published this model in 2017. The model is available here: https://github.com/phillipi/pix2pix. Their model was adapted into a notebook by Aniket Maurya, available here: https://librecv.github.io/blog/gans/pytorch/2021/02/13/Pix2Pix-explained-with-code.html. The research and notebook above is what the following code is based off and adapted from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "import os\n",
    "from glob import glob \n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset #iterates over the dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import center_crop\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm.auto import tqdm #makes our loops show a smart progress meter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code was originally in the notebook created by Aniket Maurya. This did not properly load the images, so I coded my own class directly below this. \n",
    "\n",
    "#path = \"Training_small\"  \n",
    "\n",
    "#class ArchitectDataset(Dataset): \n",
    "    #def __init__(self, path):\n",
    "      #self.filenames = glob(str(Path(path) / \"*\"))\n",
    "      #self.target_size = target_size \n",
    "\n",
    "    #def __len__(self):\n",
    "        #print(len(self.filenames))\n",
    "        #return len(self.filenames)\n",
    "    \n",
    "    #def __getitem__(self, idx):\n",
    "        #filename = self.filenames[idx]\n",
    "        #image = Image.open(filename)\n",
    "        #image = transforms.functional.to_tensor(image)\n",
    "        #image_width = image.shape[2] \n",
    "\n",
    "        #real = image[:, :, : image_width // 2] \n",
    "        #condition = image[:, :, image_width // 2 :] \n",
    "\n",
    "        #target_size = self.target_size\n",
    "        #if target_size:\n",
    "         #   condition = nn.functional.interpolate(condition, size=target_size)\n",
    "         #   real = nn.functional.interpolate(real, size=target_size)\n",
    "\n",
    "        #return real, condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a class that will separate the paired images into  real and condition images (outputs and inputs)\n",
    "#this was written in replace of the above code that did not work\n",
    "\n",
    "class Buildings_Dataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.image_files = glob('Training_small/*.png')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def read_image(self, idx):\n",
    "        file_image = self.image_files[idx]\n",
    "        image = Image.open(file_image)\n",
    "        image = np.array(image)\n",
    "        width = image.shape[1]\n",
    "        width_half = width // 2\n",
    "\n",
    "        #here there are various additions, such as swapaxes and unsqueeze used as tests to try and flip the axis produced by the class\n",
    "        #the images produced here are in the wrong order for the rest of the code to function.\n",
    "        #unfortunately, neither of these flipped the order as expected, thus the code could not move forward. \n",
    "        condition = image[:, :width_half, :]\n",
    "        condition = torch.swapaxes(condition, 0, 2)\n",
    "        # condition = torch.swapaxes(condition, 0, 1)\n",
    "        condition = condition.unsqueeze(0)\n",
    "        real = image[:, width_half:, :]\n",
    "        real = torch.swapaxes(real, 0, 2)\n",
    "        real = real.unsqueeze(0)\n",
    "        # real = torch.swapaxes(real, 0, 1)\n",
    "        print(real.shape)\n",
    "        print(condition.shape)\n",
    "\n",
    "        condition = condition.astype(np.float32)\n",
    "        real = real.astype(np.float32)\n",
    "\n",
    "        return real, condition\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking that the read_image function correctly reads in the images \n",
    "\n",
    "def read_image(path):\n",
    "    #file_image = self.image_files[idx]\n",
    "    image = Image.open(path)\n",
    "    image = np.array(image)\n",
    "    width = image.shape[1]\n",
    "    width_half = width // 2\n",
    "\n",
    "    #here there are various additions, such as swapaxes and unsqueeze used as tests to try and flip the axis produced by the class\n",
    "    #the images produced here are in the wrong order for the rest of the code to function.\n",
    "    #unfortunately, neither of these flipped the order as expected, thus the code could not move forward. \n",
    "    condition = image[:, :width_half, :]\n",
    "    #condition = torch.swapaxes(condition, 0, 2)\n",
    "    #condition = torch.swapaxes(condition, 0, 1)\n",
    "    real = image[:, width_half:, :]\n",
    "    #real = torch.swapaxes(real, 0, 2)\n",
    "    ## real = torch.swapaxes(real, 0, 1)\n",
    "    print(real.shape)\n",
    "    print(condition.shape)\n",
    "\n",
    "    condition = condition.astype(np.float32)\n",
    "    real = real.astype(np.float32)\n",
    "\n",
    "    return real, condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3)\n",
      "(512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "#running my dataset through the class above to take a look at the tensors produced\n",
    "#notice how the tensor is structued as (512, 512, 3) for the rest of the code to work, it needs to be (3, 512, 512)\n",
    "#when running torch.swapaxes(), we get the following error \"argument 'input' (position 1) must be Tensor, not numpy.ndarray\"\n",
    "#attempting to convert it to a tensor, causes other sections of the code to break\n",
    "path = \"Training_small/0002.png\"  \n",
    "image_dataset = read_image(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decrease the layer size (as part of the U-Net shaped network Pix2Pix needs)\n",
    "\n",
    "class DownSampleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel=4, strides=2, padding=1, activation=True, batchnorm=True):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.batchnorm = batchnorm\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel, strides, padding)\n",
    "\n",
    "        if batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        if activation:\n",
    "            self.act = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.batchnorm:\n",
    "            x = self.bn(x)\n",
    "        if self.activation:\n",
    "            x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expanding the network (as part of the U-Net shaped network Pix2Pix needs)\n",
    "class UpSampleConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel=4,\n",
    "        strides=2,\n",
    "        padding=1,\n",
    "        activation=True,\n",
    "        batchnorm=True,\n",
    "        dropout=False \n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.batchnorm = batchnorm\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel, strides, padding)\n",
    "\n",
    "        if batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        if activation:\n",
    "            self.act = nn.ReLU(True)\n",
    "\n",
    "        if dropout:\n",
    "            self.drop = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.deconv(x)\n",
    "        if self.batchnorm:\n",
    "            x = self.bn(x)\n",
    "\n",
    "        if self.dropout:\n",
    "            x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating Generator\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        #encoder/downsample convs\n",
    "        self.encoders=[\n",
    "            DownSampleConv(in_channels, 64, batchnorm=False),\n",
    "            DownSampleConv(64, 128),\n",
    "            DownSampleConv(128, 256),\n",
    "            DownSampleConv(256, 512),\n",
    "            DownSampleConv(512, 512),\n",
    "            DownSampleConv(512, 512),\n",
    "            DownSampleConv(512, 512), \n",
    "            DownSampleConv(512, 512, batchnorm=False),\n",
    "        ]\n",
    "\n",
    "        #decoder/upsample convs\n",
    "        self.decoders=[\n",
    "            UpSampleConv(512, 512, dropout=True),\n",
    "            UpSampleConv(1024, 512, dropout=True),\n",
    "            UpSampleConv(1024, 512, dropout=True),\n",
    "            UpSampleConv(1024, 512),\n",
    "            UpSampleConv(1024, 256),\n",
    "            UpSampleConv(512, 128),\n",
    "            UpSampleConv(256, 64),\n",
    "        ]\n",
    "\n",
    "        self.decover_channels = [512, 512, 512, 512, 256, 128, 64]\n",
    "        self.final_conv = nn.ConvTranspose2d(64, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.encoders = nn.ModuleList(self.encoders)\n",
    "        self.decoders = nn.ModuleList(self.decoders)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips_cons = []\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "\n",
    "            skips_cons.append(x)\n",
    "\n",
    "        skips_cons = list(reversed(skips_cons[:-1]))\n",
    "        decoders = self.decoders[:-1]\n",
    "\n",
    "        for decoder, skip in zip(decoders, skips_cons):\n",
    "            x = decoder(x)\n",
    "            x = torch.cat((x, skip), axis=1)\n",
    "\n",
    "        x = self.decoders[-1](x)\n",
    "        x = self.final_conv(x)\n",
    "        return self.tanh(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a discriminator using the same downsampleconv we definind in the generator\n",
    "\n",
    "class PatchGAN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels):\n",
    "        super().__init__()\n",
    "        self.d1 = DownSampleConv(input_channels, 64, batchnorm=False)\n",
    "        self.d2 = DownSampleConv(64, 128)\n",
    "        self.d3 = DownSampleConv(128, 256)\n",
    "        self.d4 = DownSampleConv(256, 512)\n",
    "        self.final = nn.Conv2d(512, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat((x, y)) #, axis=1)\n",
    "        x0 = self.d1(x)\n",
    "        x1 = self.d2(x0)\n",
    "        x2 = self.d3(x1)\n",
    "        x3 = self.d4(x2)\n",
    "        xn = self.final(x3)\n",
    "\n",
    "        return xn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the loss function\n",
    "\n",
    "adversarial_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "reconstruction_loss = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing weights\n",
    "\n",
    "def _weights_init(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def display_progress(cond, fake, real, figsize=(10,5)):\n",
    "    cond = cond.detach().cpu().permute(1, 2, 0) \n",
    "    fake = fake.detach().cpu().permute(1, 2, 0)\n",
    "    real = real.detach().cpu().permute(1, 2, 0)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=figsize)\n",
    "    ax[0].imshow(cond)\n",
    "    ax[2].imshow(fake)\n",
    "    ax[1].imshow(real)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the Pix2Pix model\n",
    "\n",
    "class Pix2Pix(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, learning_rate=0.0002, lambda_recon=200, display_step=25):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.display_step = display_step\n",
    "        self.gen = Generator(in_channels, out_channels)\n",
    "        self.patch_gan = PatchGAN(in_channels + out_channels)\n",
    "\n",
    "        #initializing weights\n",
    "        self.gen = self.gen.apply(_weights_init)\n",
    "        self.patch_gan = self.patch_gan.apply(_weights_init)\n",
    "\n",
    "        self.adversarial_criterion = nn.BCEWithLogitsLoss()\n",
    "        self.recon_criterion = nn.L1Loss()\n",
    "\n",
    "    def _gen_step(self, real_images, conditioned_images):\n",
    "        #calculate the adversarial loss\n",
    "        fake_images = self.gen(conditioned_images)\n",
    "        disc_logits = self.patch_gan(fake_images, conditioned_images)\n",
    "        adversarial_loss = self.adversarial_criterion(disc_logits, torch.ones_like(disc_logits))\n",
    "\n",
    "        #calculate the reconstruction loss\n",
    "        recon_loss = self.recon_criterion(fake_images, real_images)\n",
    "        lambda_recon = self.hparams.lambda_recon\n",
    "\n",
    "    def _disc_step(self, real_images, conditioned_images):\n",
    "        fake_images = self.gen(conditioned_images).detach \n",
    "        fake_logits = self.patch_gan(fake_images, conditioned_images)\n",
    "\n",
    "        real_logits = self.patch_gan(real_images, conditioned_images)\n",
    "\n",
    "        fake_loss = self.adversarial_criterion(fake_logits, torch.zeros_like(fake_logits))\n",
    "        real_loss = self.adversarial_criterion(real_logits, torch.ones_like(real_logits))\n",
    "        return(real_loss + fake_loss)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.learning_rate\n",
    "        gen_opt = torch.optim.Adam(self.gen.parameters(), lr=lr)\n",
    "        disc_opt = torch.optim.Adam(self.patch_gan.parameters(), lr=lr)\n",
    "        return disc_opt, gen_opt\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        real, condition = batch\n",
    "\n",
    "        loss = None\n",
    "        if optimizer_idx == 0:\n",
    "            loss = self._disc_step(real, condition)\n",
    "            self.log('PatchGAN Loss', loss)\n",
    "        elif optimizer_idx == 1:\n",
    "            loss = self._gen_step(real, condition)\n",
    "            self.log('Generator Loss', loss)\n",
    "\n",
    "        if self.current_epoch%self.display_step==0 and batch_idx==0 and optimizer_idx==1:\n",
    "            fake = self.gen(condition).detach()\n",
    "            display_progress(condition[0], fake[0], real[0])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/marissabeaty/miniconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name                  | Type              | Params\n",
      "------------------------------------------------------------\n",
      "0 | gen                   | Generator         | 54.4 M\n",
      "1 | patch_gan             | PatchGAN          | 2.8 M \n",
      "2 | adversarial_criterion | BCEWithLogitsLoss | 0     \n",
      "3 | recon_criterion       | L1Loss            | 0     \n",
      "------------------------------------------------------------\n",
      "57.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "57.2 M    Total params\n",
      "228.713   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afef1d67f20f4ecdb15cc3ba02665937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/marissabeaty/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/marissabeaty/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'Buildings_Dataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/marissabeaty/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/marissabeaty/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'Buildings_Dataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/marissabeaty/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/marissabeaty/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'Buildings_Dataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/marissabeaty/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/marissabeaty/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'Buildings_Dataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/marissabeaty/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/marissabeaty/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'Buildings_Dataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/marissabeaty/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/marissabeaty/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'Buildings_Dataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/marissabeaty/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/marissabeaty/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'Buildings_Dataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/marissabeaty/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/marissabeaty/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'Buildings_Dataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 74234) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1121\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    114\u001b[0m     \u001b[39mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/connection.py:429\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 429\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    430\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    937\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[39m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     _error_if_any_worker_fails()\n\u001b[1;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m previous_handler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 74234) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m pix2pix \u001b[39m=\u001b[39m Pix2Pix(\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[1;32m      8\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m) \n\u001b[0;32m----> 9\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(pix2pix, dataloader)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    607\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    609\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    610\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    644\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    652\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1112\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1114\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1190\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1191\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   1213\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1214\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:267\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(dataloader, batch_to_device\u001b[39m=\u001b[39mbatch_to_device)\n\u001b[1;32m    266\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:187\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_fetcher, DataLoaderIterDataFetcher):\n\u001b[1;32m    186\u001b[0m     batch_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 187\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(data_fetcher)\n\u001b[1;32m    188\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     batch_idx, batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(data_fetcher)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py:184\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetching_function()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py:265\u001b[0m, in \u001b[0;36mDataFetcher.fetching_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    263\u001b[0m     \u001b[39m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader_iter)\n\u001b[1;32m    266\u001b[0m         \u001b[39m# consume the batch we just fetched\u001b[39;00m\n\u001b[1;32m    267\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py:280\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    278\u001b[0m start_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_fetch_start()\n\u001b[1;32m    279\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[1;32m    281\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    282\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop_profiler()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py:569\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    564\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fetches the next batch from multiple data loaders.\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \n\u001b[1;32m    566\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[39m        a collections of batch data\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader_iters)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py:581\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.request_next_batch\u001b[0;34m(loader_iters)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    572\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest_next_batch\u001b[39m(loader_iters: Union[Iterator, Sequence, Mapping]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    573\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the batch of data from multiple iterators.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \n\u001b[1;32m    575\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[39m        Any: a collections of batch data\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m     \u001b[39mreturn\u001b[39;00m apply_to_collection(loader_iters, Iterator, \u001b[39mnext\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/lightning_utilities/core/apply_func.py:51\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m# Breaking condition\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, dtype) \u001b[39mand\u001b[39;00m (wrong_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     53\u001b[0m elem_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(data)\n\u001b[1;32m     55\u001b[0m \u001b[39m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1315\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1316\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1319\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1282\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1281\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1282\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1283\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1284\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(failed_workers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1132\u001b[0m     pids_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(w\u001b[39m.\u001b[39mpid) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1133\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mDataLoader worker (pid(s) \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) exited unexpectedly\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(pids_str)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, queue\u001b[39m.\u001b[39mEmpty):\n\u001b[1;32m   1135\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 74234) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "#train the model\\\n",
    "batch_size = 2\n",
    "image_dataset = Buildings_Dataset('')\n",
    "#added num workers here based on research of errors not defining num workers. \n",
    "dataloader = DataLoader(image_dataset, batch_size=batch_size, shuffle=True, num_workers=8) \n",
    "\n",
    "pix2pix = Pix2Pix(3, 3)\n",
    "trainer = pl.Trainer(max_epochs=10, accelerator=\"cpu\") \n",
    "trainer.fit(pix2pix, dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to train the model, I kept getting the same error: \"DataLoader worker (pid(s) 74234) exited unexpectedly.\" After research into this, I concluded that this was an issue with the DataLoader not being able to read the image files. This goes back to the error we get above, in that the photos themselves are read in incorrectly for the model. Specifically, they are read in as width, height, RGB, but for the model to work they need to be formatted as RGB, width, height. Attempting to change them to the correct format caused other sections of the code to break. No other solutions were found to fix this error. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:27:35) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "867332fcff6d9ef60802a7aa280ca2e67d2157da374b31f12b9e31ff4c97c0fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
